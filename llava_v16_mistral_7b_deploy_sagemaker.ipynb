{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d942e395-2ebf-400d-8c0d-aaed37da9672",
   "metadata": {},
   "source": [
    "## Deploy Llava 1.6 with Mistral 7B Endpoint on SageMaker\n",
    "\n",
    "In this notebook, we will deploy a Llava 1.6 endpoint with DJLServing container image.\n",
    "\n",
    "This notebook has been tested within SageMaker Studio Notebook Instance with pytorch environment. \n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c504d1b-fb05-432b-ab0f-fbded0f0dc14",
   "metadata": {},
   "source": [
    "This notebook has been tested on Amazon SageMaker Notebook Instances with single GPU on ml.g5.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cce6d006-3e03-4773-810f-1e593c757b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub transformers==4.38.1 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6f1ecae5-822a-487c-a58f-3a6f364eb508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import json\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bdfc4d04-25f9-4c10-aadf-a76b57ba8cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f55e63f5-ab27-4eb1-81ca-2e166483f1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model will be uploaded to ---- > s3://sagemaker-us-east-1-70768*******/model-llava16/\n"
     ]
    }
   ],
   "source": [
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = \"llava16-code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = \"model-llava16\"  # folder within bucket where code artifact will go\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "# define a variable to contain the s3url of the location that has the model\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "print(f\"Pretrained model will be uploaded to ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7499bb-2944-4bdc-95a3-fd3ec3dee465",
   "metadata": {},
   "source": [
    "## Prepare inference script and container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "47ee875d-91d0-4ab8-901f-0a0bce95652b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.25.0-deepspeed0.11.0-cu118'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=sess.boto_session.region_name, version=\"0.25.0\"\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c3412-1df2-49d2-9cd0-31b2326389c4",
   "metadata": {},
   "source": [
    "In this notebook, we will provide two ways to load the model when deploying to an endpoint.\n",
    "- Directly load from Hugging Face \n",
    "- Store the model artifacts on S3 and load the model directly from S3\n",
    "\n",
    "The [Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) container uses [s5cmd](https://github.com/peak/s5cmd) to download data from S3 which significantly reduces the speed when loading model during deployment. Therefore, we recommend to load the model from S3 by following the below section to download the model from Hugging Face and upload the model on S3. \n",
    "\n",
    "If you choose to load the model directly from Hugging Face during model deployment, you can skip the below section and jump to the section to **prepare the model tarbal file and upload to S3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7102d7-58e5-449e-8dc3-eb1bf76da79a",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "If you intend to download your copy of the model and upload it to a s3 location in your AWS account, please follow the below steps, else you can skip to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aaf38b0-5006-40e0-95be-8b1bbea48ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c285b3f13506495abb42a6c0b68c0d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60447c4402b2466c8a9771513f191018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a5f86342d341649e917820b7b4c820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609967f7086a4b6e8f30922977c1081c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90d6b1d46af4cb29b03d241ed9e28d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda3fc001f0543d39386207b1c26a364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1146491922b4460a9dadeb5a6ccda3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768b48bc6d2e46c69a5d6ffd55119fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f8799e8690464b8cb1177abee43330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/744 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1591a929584c2989c785d7e467c414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477be252ddc84a6ea14f7671706d6cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d831c3a3ea47499f58438b5f31d557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2b091ac6554ec598c192762261ad7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7ca4c1aa53491daa7ad55a3655cd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\"./llava-model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\", \"*.safetensors\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4ed9da85-11f5-4dfd-8745-f05dcda5df53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llava-model/models--llava-hf--llava-v1.6-mistral-7b-hf/snapshots/4dbf61a5df5b38ec222b3acb12a947b5cef29312'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_download_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2be006-576f-4799-a16c-f26a9da66b80",
   "metadata": {},
   "source": [
    "Please make sure the file is downloaded correctly by checking the files exist in the newly created folder `blip2-model/models--Salesforce--<model-name>/snapshots/...` before running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5ec0aeb4-9e76-4b4b-ab69-f3aacecb32b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to --- > s3://sagemaker-us-east-1-70768*******/model-llava16\n",
      "We will set option.s3url=s3://sagemaker-us-east-1-70768*******/model-llava16\n"
     ]
    }
   ],
   "source": [
    "# upload the model artifacts to s3\n",
    "model_artifact = sess.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")\n",
    "print(f\"We will set option.s3url={model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "79b8532e-ee56-4596-a55a-0666baac42a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!rm -rf {local_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823c93a-4bc8-44f9-b4c2-b8f137e9c46d",
   "metadata": {},
   "source": [
    "SageMaker Large Model Inference containers can be used to host models without providing your own inference code. This is extremely useful when there is no custom pre-processing of the input data or post-processing of the model's predictions.\n",
    "\n",
    "However, in this notebook, we demonstrate how to deploy a model with custom inference code.\n",
    "\n",
    "SageMaker needs the model artifacts to be in a Tarball format. In this example, we provide the following files - `serving.properties`, `model.py`, and `requirements.txt`.\n",
    "- `serving.properties` is the configuration file that can be used to indicate to DJL Serving which model parallelization and inference optimization libraries you would like to use. Depending on your need, you can set the appropriate configuration. For more details on the configuration options and an exhaustive list, you can refer the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html).\n",
    "- `model.py` is the script handles any requests for serving.\n",
    "- `requirements.txt` is the text file containing any additional pip wheel need to install. \n",
    "\n",
    "If you want to download the model from huggingface.co, you can set option.model_id. The model id of a pretrained model hosted inside a model repository on huggingface.co (https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co. If you set the model_id to a s3 url, the DJL will download the model artifacts from s3 and swap the model_id to the actual location of the model artifacts. In your script, you can point to this value to load the pre-trained model.\n",
    "- `option.tensor_parallel_degree`: Set to the number of GPU devices over which the model needs to be partitioned. This parameter also controls the number of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine, and we are creating 8 partitions then we will have 1 worker per model to serve the requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "001f38dc-1d75-4167-a4f9-4518024c7bef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llava-djl-config/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile llava-djl-config/serving.properties\n",
    "engine = Python\n",
    "option.tensor_parallel_degree = 1\n",
    "option.model_loading_timeout = 600\n",
    "option.model_id = {{s3url}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "19eabd92-12d6-4f05-90fe-219c290e0b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mPython\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[36moption.model_loading_timeout\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m600\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[36moption.model_id\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33ms3://sagemaker-us-east-1-70768*******/model-llava16/\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(\"llava-djl-config/serving.properties\").open().read())\n",
    "Path(\"llava-djl-config/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=pretrained_model_location)\n",
    ")\n",
    "!pygmentize llava-djl-config/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c66dd-ee1d-4cc5-961b-0b9b063c4f22",
   "metadata": {},
   "source": [
    "## Prepare the model tarball file and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "eeae8bc2-f3c9-4174-8ebe-3230efaa72a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llava-djl-config/\n",
      "llava-djl-config/model.py\n",
      "llava-djl-config/requirements.txt\n",
      "llava-djl-config/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tar czvf model.tar.gz llava-djl-config/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8ec0e818-1a0a-4d5a-b86e-f06d19eaa0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-70768*******/llava16-code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fb573-b45c-4bbc-8f2e-0e46bff6f14a",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "304c8205-c549-4f09-abab-7525047fab39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "deploy_model_name = \"llava-v16-mistral-7b-hf\"\n",
    "model = Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=s3_code_artifact,\n",
    "    role=role,\n",
    "    name=deploy_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "52fc1232-8d70-4ea1-a617-d43fb4eaab3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: llava-v16-mistral-7b-hf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!CPU times: user 133 ms, sys: 0 ns, total: 133 ms\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "endpoint_name = \"endpoint-\" + deploy_model_name\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "55b1b633-2839-42c4-a4e0-65568e8bd302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'endpoint-llava-v16-mistral-7b-hf'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1593b3-0ba7-4936-9338-6f1e5dacbb26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af32882-3587-4832-9948-36d36d87b050",
   "metadata": {},
   "source": [
    "## Test Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3987ea64-7038-4020-8144-8895fd072b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "endpoint_name = model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "dae73ace-0aea-4379-9083-471b168d62bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_image(img_file):\n",
    "    with open(img_file, \"rb\") as image_file:\n",
    "        img_str = base64.b64encode(image_file.read())\n",
    "        base64_string = img_str.decode(\"latin1\")\n",
    "    return base64_string\n",
    "\n",
    "def run_inference(endpoint_name, inputs):\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, Body=json.dumps(inputs)\n",
    "    )\n",
    "    return response[\"Body\"].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "206575f9-6b7e-40b3-92c7-3296b9df1618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image = \"food.png\"\n",
    "raw_image = Image.open(test_image).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ed3a3-d231-4f7b-81f6-b1f9fe38678b",
   "metadata": {},
   "source": [
    "### You can check out the food image and the generated description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "aad17e6c-a40a-4cd0-a2c8-d179fa1594ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display(raw_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550a4af-9202-4525-b8f9-7047edd3ba4c",
   "metadata": {},
   "source": [
    "### Instructed zero-shot vision-to-language detailed caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e1d2e11b-7cee-4790-967a-100084039a73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"USER: What's the content of the image in details?\\nASSISTANT: The image shows a close-up view of a pizza with various toppings. The pizza has a golden-brown crust and is generously covered with melted cheese. The toppings include black olives, which are scattered across the pizza, and green peppers, which are sliced and distributed evenly. There are also pieces of sausage, which appear to be browned and cooked. The pizza is placed on a wooden surface, and in the background, there are other food items, including what looks like a tomato and a leafy green vegetable, possibly arugula or spinach. The overall presentation suggests a freshly made pizza, ready to be enjoyed.\""
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64_string = encode_image(test_image)\n",
    "\n",
    "prompt = \"<image>\\nUSER: What's the content of the image in details?\\nASSISTANT:\"\n",
    "\n",
    "param = {\n",
    "    \"max_new_tokens\": 500,\n",
    "}\n",
    "\n",
    "\n",
    "inputs = {\"prompt\": prompt, \"image\": base64_string, \"parameters\": param}\n",
    "run_inference(endpoint_name, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f1d43-0746-4877-91de-5eaaa6594888",
   "metadata": {},
   "source": [
    "### Instructed Visual Question Answering (VQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "30740220-5554-40d2-aea4-0e3862819099",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"USER: Is it recommended for eating before sport? \\nASSISTANT: The image shows a pizza with various toppings, including olives, onions, and possibly meat. While pizza can be a delicious meal, it's generally not recommended as a pre-sport meal due to its high calorie and fat content. Consuming a heavy meal before physical activity can lead to discomfort and sluggishness during exercise. Instead, it's better to opt for lighter, easily digestible foods that provide energy without causing discomfort. Examples include a banana, a granola bar, or a small salad.\""
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64_string = encode_image(test_image)\n",
    "\n",
    "prompt = \"<image>\\nUSER: Is it recommended for eating before sport? \\nASSISTANT:\"\n",
    "\n",
    "param = {\n",
    "    \"max_new_tokens\": 500,\n",
    "}\n",
    "\n",
    "\n",
    "inputs = {\"prompt\": prompt, \"image\": base64_string, \"parameters\": param}\n",
    "run_inference(endpoint_name, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c641b993-ff23-4010-8504-65639f27e3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Can you list several descriptions for product marketing? \n",
      "ASSISTANT: Certainly! Here are several descriptions that could be used for product marketing:\n",
      "\n",
      "1. \"Indulge in the ultimate pizza experience with our gourmet pizza, topped with a variety of fresh ingredients and served on a crispy, golden crust.\"\n",
      "2. \"Savor the flavors of the Mediterranean with our authentic Greek pizza, featuring a rich tomato sauce, succulent chicken, and a blend of feta and mozzarella cheeses.\"\n",
      "3. \"Experience the taste of Italy with our classic Margherita pizza, made with the finest mozzarella, ripe tomatoes, and fresh basil leaves.\"\n",
      "4. \"Enjoy a hearty meal with our meat lovers pizza, loaded with pepperoni, sausage, bacon, and ground beef, all nestled on a perfectly baked crust.\"\n",
      "5. \"Treat yourself to a vegetarian feast with our garden pizza, topped with a colorful array of vegetables including bell peppers, onions, mushrooms, and olives.\"\n",
      "6. \"Satisfy your cravings with our supreme pizza, featuring a generous assortment of toppings including pepperoni, sausage, mushrooms, onions, and green peppers.\"\n",
      "7. \"Experience the flavors of the sea with our seafood pizza, topped with shrimp, anchovies, and calamari, all served on a perfectly baked crust.\"\n",
      "8. \"Indulge in a delicious and satisfying meal with our BBQ chicken pizza, topped with succulent chicken, red onions, and a tangy BBQ sauce.\"\n",
      "9. \"Enjoy a taste of the tropics with our Hawaiian pizza, featuring a sweet and savory combination of ham, pineapple, and bacon.\"\n",
      "10. \"Savor the flavors of the Southwest with our Southwest pizza, topped with grilled chicken, black beans, corn, and jalapenos.\"\n",
      "\n",
      "These descriptions highlight the variety of flavors and toppings available, appealing to different tastes and preferences.\n"
     ]
    }
   ],
   "source": [
    "base64_string = encode_image(test_image)\n",
    "\n",
    "prompt = \"<image>\\nUSER: Can you list several descriptions for product marketing? \\nASSISTANT:\"\n",
    "\n",
    "param = {\n",
    "    \"max_new_tokens\": 500,\n",
    "}\n",
    "\n",
    "\n",
    "inputs = {\"prompt\": prompt, \"image\": base64_string, \"parameters\": param}\n",
    "print(run_inference(endpoint_name, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c714b1b-00c6-48be-b0d0-2381bf5e6cf2",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Uncomment the below cell to delete the endpoint and model when you finish the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "22616f1c-a919-4f77-af58-de3a0bbffe06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sm_client.delete_model(ModelName=deploy_model_name)\n",
    "#sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7d3ff-5ae5-4aa1-9f29-bf71abe4dcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
