{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6e30a1-2e14-476f-ab61-550756341bef",
   "metadata": {},
   "source": [
    "### Huggingface version of Llava 1.6 with Mistral 7b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc93dda-8876-43a0-9563-538e012211f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.38.1 accelerate --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127d9a5-8c35-4be1-81b0-6ad2c9a70a38",
   "metadata": {},
   "source": [
    "The LLaVa model was proposed in Visual Instruction Tuning and improved in Improved Baselines with Visual Instruction Tuning by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee. \n",
    "\n",
    "The Huggingface SDK is provided here https://huggingface.co/docs/transformers/en/model_doc/llava"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fca3b4-899e-4253-a2f1-acf17669f469",
   "metadata": {},
   "source": [
    "This notebook has been tested on Amazon SageMaker Notebook Instances with single GPU on ml.g5.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fababa2e-a571-41ae-ae0f-2b6402a91486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224b98c910d04e7490e49450dab26cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at llava-hf/llava-v1.6-mistral-7b-hf were not used when initializing LlavaForConditionalGeneration: ['image_newline']\n",
      "- This IS expected if you are initializing LlavaForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch \n",
    "\n",
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "# you can also load from local file\n",
    "#model_id = \"llava-model/models--llava-hf--llava-v1.6-mistral-7b-hf/snapshots/4dbf61a5df5b38ec222b3acb12a947b5cef29312\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, device_map=device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1ce09c-23bf-45d7-8d12-12d518e75bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaProcessor:\n",
      "- image_processor: CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_aspect_ratio\": \"anyres\",\n",
      "  \"image_grid_pinpoints\": [\n",
      "    [\n",
      "      336,\n",
      "      672\n",
      "    ],\n",
      "    [\n",
      "      672,\n",
      "      336\n",
      "    ],\n",
      "    [\n",
      "      672,\n",
      "      672\n",
      "    ],\n",
      "    [\n",
      "      1008,\n",
      "      336\n",
      "    ],\n",
      "    [\n",
      "      336,\n",
      "      1008\n",
      "    ]\n",
      "  ],\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: LlamaTokenizerFast(name_or_path='llava-hf/llava-v1.6-mistral-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"LlavaProcessor\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137ae063-abb5-4fc6-baf3-6582fef26071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "prompt = \"<image>\\nUSER: What's the content of the image in details?\\nASSISTANT:\"\n",
    "image = Image.open(\"food.png\")\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5de97-88d0-4250-a68f-a52091e446ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### You can check out the food image and the generated description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b654cfb9-4315-4b94-b986-ea89cbca32fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0805ff4b-37fd-4e8e-a43e-de5a8b29ac57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nUSER: What's the content of the image in details?\\nASSISTANT: The image shows a close-up view of a pizza with various toppings. The pizza has a golden-brown crust and is generously covered with melted cheese. The toppings include black olives, which are scattered across the pizza, and green peppers, which are sliced and distributed evenly. There are also pieces of sausage, which appear to be browned and cooked. The pizza is placed on a wooden surface, and in the background, there are other food items, including what looks like a tomato and a leafy green vegetable, possibly arugula or spinach. The overall presentation suggests a freshly made pizza, ready to be enjoyed. \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(**inputs,max_new_tokens = 300)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de774ed0-b93c-471d-b3d6-8f5df0a47f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
